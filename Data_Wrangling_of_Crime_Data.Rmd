---
title: 'IMT 573: Problem Set 3 - Working With Data II'
author: "Alana Montoya"
date: 'Due: Tuesday, October 26, 2021'
output:
  pdf_document: default
  html_document: default
---

<!-- This syntax can be used to add comments that are ignored during knitting process. -->

##### Collaborators: <!-- BE SURE TO LIST ALL COLLABORATORS HERE! -->

##### Instructions:

Before beginning this assignment, please ensure you have access to R and RStudio; this can be on your own personal computer or on the IMT 573 R Studio Server. 

1. Download the `problemset3.Rmd` file from Canvas or save a copy to your local directory on RStudio Server. Open `problemset3.Rmd` in RStudio and supply your solutions to the assignment by editing `problemset3.Rmd`. 

2. Replace the "Insert Your Name Here" text in the `author:` field with your own full name. Any collaborators must be listed on the top of your assignment. 

4. All materials and resources that you use (with the exception of lecture slides) must be appropriately referenced within your assignment. In particular, note that Stack Overflow is licenses as Creative Commons (CC-BY-SA). This means you have to attribute any code you refer from SO.

5. Partial credit will be awarded for each question for which a serious attempt at finding an answer has been shown. But please **DO NOT** submit pages and pages of hard-to-read code and attempts that is impossible to grade. That is, avoid redundancy. Remember that one of the key goals of a data scientist is to produce coherent reports that others can easily follow.  Students are \emph{strongly} encouraged to attempt each question and to document their reasoning process even if they cannot find the correct answer. If you would like to include R code to show this process, but it does not run without errors you can do so with the `eval=FALSE` option as follows:

```{r example chunk with a bug, eval=FALSE}
a + b # these object dont' exist 
# if you run this on its own it with give an error
```

6. When you have completed the assignment and have **checked** that your code both runs in the Console and knits correctly when you click `Knit PDF`, rename the knitted PDF file to `ps3_ourLastName_YourFirstName.pdf`, and submit the PDF file on Canvas.

7.  Collaboration is often fun and useful, but each student must turn in an individual write-up in their own words as well as code/work that is their own.  Regardless of whether you work with others, what you turn in must be your own work; this includes code and interpretation of results. The names of all collaborators must be listed on each assignment. Do not copy-and-paste from other students' responses or code.


##### Setup: 

In this problem set you will need, at minimum, the following R packages.

```{r Setup, message=FALSE, warning=FALSE}
# Load standard libraries
library('dplyr')
library('stringr')
library('ggplot2')
library('tidyverse')
library('tigris')
```

#### Problem 1: Joining Census Data to Police Reports

In this problem set, we will be joining disparate sets of data - namely: Seattle police crime data, information on Seattle police beats, and education attainment from the US Census. Our goal is to build a dataset where we can examine questions around crimes in Seattle and the educational attainment of people living in the areas in which the crime occurred; this requires data to be combined from these two individual sources.

As a general rule, be sure to keep copies of the original dataset(s) as you work through cleaning (remember data provenance!).

##### (a) Importing and Inspecting Crime Data

Load the Seattle crime data from the provided `crime_data.csv` data file. You can find more information on the data here:  \url{https://data.seattle.gov/Public-Safety/Crime-Data/4fs7-3vj5}. This dataset is constantly refreshed online so we will be using the provided csv file for consistency. We will call this dataset the "Crime Dataset." Perform a basic inspection of the Crime Dataset and discuss what you find.

**Answer:**

After loading the `crime_data.csv` file into the same folder as this markdown file, we can use the `read.csv()` functions to load the Seattle crime data.

```{r}
crime_data <- read.csv("crime_data.csv", stringsAsFactors = FALSE)
```

Now we can inspect the data by first using the `glimpse()` function:

```{r}
glimpse(crime_data)
```

Given that each row represents a crime, this data illustrates information about 523,591 crimes in Seattle across 10 different characteristics (excluding the `Report.Number`). Looking at the datatypes, `Occurred.Date` and `Reported.Date` should both be expressed as a date data type. `Occurred.Time` and `Reported.Time` could also be separated into hours and minutes or a time data type such as POSIX.

Next, we can apply the `summary()` function on `Occurred.Time` and `Reported.Time` since they have a numeric data type (since `Report.Number` is more of a unique idenifying code for each crime, it's not very useful to use `summary()` on it).

```{r}
numeric_crime_data <- select(crime_data, Occurred.Time, Reported.Time)

lapply(numeric_crime_data, summary)
```

From this we can see that the minimums and maximums reflect the earliest and latest times of the day, so this looks very reasonable. There are only 2 NA values out of 523,591 entries, which is quite good, though we can further inspect this to see if they there is any connection:

```{r}
filter(crime_data, is.na(Occurred.Time) | is.na(Reported.Time))
```

Since there is no date specified for when these crimes occurred, it makes since that the time it occurred could also be missing. This may have been due to the victim reporting the incident at a later date, which means they might not remember the details on the date or time in which the event occurred. It is unclear why there would be no reported time if there is still a reported date, though there could be some correlation since they were both reported on the first day of the year.

##### (b) Looking at Years That Crimes Were Committed

Let's start by looking at the years in which crimes were committed. What is the earliest year in the dataset? Are there any distinct trends with the annual number of crimes committed in the dataset?

Subset the data to only include crimes that were committed after 2011 (remember good practices of data provenance!). Going forward, we will use this data subset.

**Answer:**

In order to find the minimum, we must first isolate the year in `Occurred.Date` and convert it to a numeric value. Then we can take the minimum of that value to find the earliest year.

```{r}
# Split the day, month, and year in the occurred dates
year_split <- strsplit(crime_data$Occurred.Date, split = "/")

# Write a function to select the third element of an input
select_third <- function(x) {
  x[3]
}

# Apply `select_third` over `year_split` to select just the year values
years <- lapply(year_split, select_third)

# Convert `years` to a numeric data type
years_numeric <- as.numeric(years)

# Find the minimum year
min(years_numeric, na.rm = TRUE)
```

The minimum year is **1908**.

Now, we can create a visualization that explores the number of crimes there were each year. Before doing so, we must turn the `years_numeric` list into a tibble since a visualization cannot be made from a list.

```{r, warning = FALSE}
# Turn the `years_numeric` list into a tibble
years_tibble <- as_tibble_col(years_numeric, column_name = "year")

# Create a histogram from `years_tibble` that illustrates how many crimes were
# commited each year
ggplot(data = years_tibble) +
  geom_histogram(mapping = aes(x = year), binwidth = 1)
```

_Note: [\textcolor{blue}{this}](https://tibble.tidyverse.org/reference/as_tibble.html) source was used to learn about `as_tibble_col()`._

From this visualization, we see that there is a sharp increase in crimes starting around the mid-late 2000s. This is more likely due to more data being logged than the actual number of crimes being committed.

Now we can subset the data to only include crimes that were committed after 2011 and store it in a new data frame. 

```{r}
# Filter the data to only include crimes after the year 2011, and then store it
# in a new data frame
crimes_data_2011 <- filter(crime_data, years_numeric > 2011)
```

##### (c) Looking at Frequency of Beats

What is a Police Beat? How frequently are the beats in the Crime Dataset listed? Are there any anomolies with how frequently some of the beats are listed? Are there missing beats?

**Answer:**

According to [\textcolor{blue}{this}](https://www.seattle.gov/police/information-and-data/tweets-by-beat) source, Seattle is divided into 5 precincts: North, East, South, West, and Southwest. Within these precincts are sectors, and then within each sectors are 3 _beats_. Each beat represents an area that a patrol officer is responsible for. Beats are represented by one letter and one number. The letter represents the sector and the number represents a beat within the sector.

Next we can check the frequency that beats were listed in the Crime Dataset. First we can count how many total rows there were in `crimes_data_2011`. Then we can count how many Beat values were `NA` or empty values and subtract it from the total number of crimes. Then this number can be divided by the total number of crimes after 2011 to get the percentage that Beats were listed.

```{r}
# Calculate the total number of crimes after 2011
num_crimes <- nrow(crimes_data_2011)
num_crimes

# Calculate the number of beats that were recorded
num_beats <- num_crimes - sum(crimes_data_2011$Beat == "" | is.na(crimes_data_2011$Beat))
num_beats

# Find the percentage that Beats were recorded
(num_beats / num_crimes) * 100
```

There were a total of **347,999 beats** recorded in the Crime Dataset, which meant beats were recorded about **99.4%** of the time.

Now we can plot a visualization to view how many crimes were committed in each Beat. First we can group by Beats and then summarize to find the count of each beat. Then we can use `geom_col()` to plot the Beats on the x-axis and the number of beats on the y-axis.

```{r}

beats_count <- crimes_data_2011 %>% group_by(Beat) %>% summarise(count = n())


ggplot(data = beats_count, mapping = aes(x = Beat, y = count)) +
  geom_col() +
  labs(title = "The Frequency of Beats Seems Relatively Scattered", y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.2))
```

The Beats seem relatively scattered, though `K3`, `E2`, `U1`, and the beats in the `M` sector seem to have more crimes. There are some beats listed that are not in the typical Beat format, such as `CTY`, `DET`, `K`, `N`, `S`, `SS` and `W`. It is unclear what `CTY` and `DET` stand for, though given how few entries there are of these, they could have been a logging error. `K`, N`, `s`, `SS`, and `W` seem like the sector was recorded, but not the beat within the sector. These could either be mistakes, crimes that took place in multiple beats within a sector, or simply there was not enough information about the crime location to determine the beat specific within the sector.

Now we can look at the number of missing Beats. There also appeared to be empty values above, so we can count those as well.

```{r}
# Count the number of missing values
sum(is.na(crimes_data_2011$Beat))

# Count the number of empty strings
sum(crimes_data_2011$Beat == "")
```

There were **0 missing Beats** and **2054 empty Beats**.

##### (d) Importing Police Beat Data and Filtering on Frequency

Load the data on Seattle police beats  provided in `police_beat_and_precinct_centerpoints.csv`. You can find additional information on the data here: (https://data.seattle.gov/Land-Base/Police-Beat-and-Precinct-Centerpoints/4khs-fz35). We will call this dataset the "Beats Dataset."

Does the Crime Dataset include police beats that are not present in the Beats Dataset? If so, how many and with what frequency do they occur? Would you say that these comprise a large number of the observations in the Crime Dataset or are they rather infrequent? Do you think removing them would drastically alter the scope of the Crime Dataset?

Let's remove all instances in the Crime Dataset that have beats which occur fewer than 10 times across the Crime Dataset. Also remove any observations with missing beats. After only keeping years of interest and filtering based on frequency of the beat, how many observations do we now have in the Crime Dataset?

**Answer:**

After loading the `police_beat_and_precinct_centerpoints.csv` file into the same folder as this markdown file, we can use the `read.csv()` function to load in the "Beats Dataset`.

```{r}
beats_data <-  read.csv("police_beat_and_precinct_centerpoints.csv",
                        stringsAsFactors = FALSE)
```

Now, to see if there were police beats that were present in the Crime Dataset but not in the Beats Dataset, we can first find the unique beats in each of the datasets seperately. Then we can use the `setdiff()` function to find which beats were the Crime Dataset, but not in the Beats Dataset.

```{r}
# Find the unique beats in the Crimes Dataset
unique_beats_crime_data <- unique(crimes_data_2011$Beat)

# Find the unique beats in the Beats Dataset
unique_beats_beats_data <- unique(beats_data$Name)

# Find the beats that were in the Crime Dataset, but not in the Beats Dataset
beats_in_crimes_not_beats <- setdiff(unique_beats_crime_data,
                                     unique_beats_beats_data)

# Find the number of beats in `beats_in_crimes_not_beats`
length(beats_in_crimes_not_beats)

# Find the frequency of each beat in `beats_in_crimes_not_beats` in the
# `crimes_data_2011` dataset
beat_freq_uniq_beats_in_crimes <- crimes_data_2011 %>%
  filter(Beat %in% beats_in_crimes_not_beats) %>%
  group_by(Beat) %>%
  summarise(count = n())
beat_freq_uniq_beats_in_crimes

(sum(beat_freq_uniq_beats_in_crimes$count) / nrow(crimes_data_2011)) * 100
```

From this we can see that there were 6 beats in the Crime Dataset that were not in the Beats Dataset (excluding empty values). "" occurs 2054 times, `CTY` occurs 1 time, `DET` occurs 7 times, `K` occurs 1 time, `S` occurs 4 times, `SS` occurs 1 time, and `WS` occurs 1 time. These beats (including the empty values) comprise less than 1% of the observations in `crimes_data_2011`, so they are rather infrequent. Given this, removing them would NOT drastically alter the scope of the Crime Dataset.

Now let's remove the beats in the Crimes Dataset that have beats which occur fewer than 10 times and observations with missing (or empty) beats.

```{r}
# Filter the beats in `beats_count` to remove beats which occur fewer than 10
# times 
beats_count_greater_eq_10 <- beats_count %>% filter(count >= 10)

# Create vector of Beats that occur greater than or equal to 10 times
beats_greater_eq_10 <- beats_count_greater_eq_10$Beat

# Remove instances that have beats which occur fewer than 10 times across the
# Crime Dataset
trimmed_crimes_data_2011 <- crimes_data_2011 %>%
  filter(Beat %in% beats_greater_eq_10)

# Remove Remove instances that empty beat values
trimmed_crimes_data_2011 <- trimmed_crimes_data_2011 %>% filter(Beat != "")

# Find the number of observations in the filtered Crime Dataset
nrow(trimmed_crimes_data_2011)
```

Now there are only **347,980 observations** in the Crime Dataset.

##### (e) Importing and Inspecting Police Beat Data

To join the Beat Dataset to census data, we must have census tract information. Use the `censusr` package to extract the 15-digit census tract for each police beat using the corresponding latitude and longitude. Do this using each of the police beats listed in the Beats Dataset. Do not use a for-loop for this but instead rely on R functions (e.g. the 'apply' family of functions). Add a column to the Beat Dataset that contains the 15-digit census tract for the each beat. (HINT: you may find `censusr`'s `call_geolocator_latlon` function useful)

We will eventually join the Beats Dataset to the Crime Dataset. We could have joined the two and then found the census tracts for each beat. Would there have been a particular advantage/disadvantage to doing this join first and then finding census tracts? If so, what is it? (NOTE: you do not need to write any code to answer this)

**Answer:**

We can extract the 15-digit census tract codes for each police beat by using the `mapply()` function to apply the `call_geolocator_latlon()` function to each row in `beats_data` and assigning it to the variable `Census.Tract` in `beats_data`.

```{r}
beats_data['Census.Tract'] <- mapply(call_geolocator_latlon,
                                     lat = beats_data$Latitude,
                                     lon = beats_data$Longitude)
```

A possible advantage to doing this join first and then finding census tracts is that you could avoid extracting the 15-digit for beats in the Beat Dataset that are not included in the Crime Dataset, that is, you would only perform the `call_geolocator_latlon` function on only the beats you need from Crime Dataset. A disadvantage to doing the join first would be that there could be rows in the datasets that could be left out.

##### (f) Extracting FIPS Codes

Once we have the 15-digit census codes, we will break down the code based on information of interest. You can find more information on what these 15 digits represent here: https://transition.fcc.gov/form477/Geo/more_about_census_blocks.pdf.

First, create a column that contains the state code for each beat in the Beats Dataset. Then create a column that contains the county code for each beat. Find the FIPS codes for WA State and King County (the county of Seattle) online. Are the extracted state and county codes what you would expect them to be? Why or why not?

**Answer:**

According to the provided website, the first two numbers in the census code is the state code and the next three numbers is the county code within the state. So, we can make two functions, one that gets a state code and one that gets a county code when provided with a census code. Then we can apply each to the `Census.Tract` column in the `beats_data` dataset and use mutate to add them as new columns.

```{r}
# STATE CODES

# Make function that gets a state code when provided a census code. First split
# the census code to separate each number. Then return the first and second
# numbers in the census code.
get_state_code <- function(census_tract_val) {
  split <- strsplit(census_tract_val, split = "")
  paste0(split[[1]][1], split[[1]][2])
}

# Apply the `get_state_code()` to each `Census.Tract` value in the `beats_data`
# dataset
state_code <- lapply(beats_data$Census.Tract, FUN = get_state_code)

# COUNTY CODES

# Make function that gets a county code when provided a census code. First split
# the census code to separate each number. Then return the third, fourth and
# fifth numbers in the census code.
get_county_code <- function(census_tract_val) {
  split <- strsplit(census_tract_val, split = "")
  paste0(split[[1]][3], split[[1]][4], split[[1]][5])
}

# Apply the `get_county_code()` to each `Census.Tract` value in the `beats_data`
# dataset
county_code <- lapply(beats_data$Census.Tract, FUN = get_county_code)

# Add `state_code` and `county_code` as new columns in `beats_data`
beats_data <- mutate(beats_data, State.Code = state_code,
                     County.Code = county_code)
```

According to [\textcolor{blue}{this}](https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt) source, the state and county codes are what I would have expected them to be. The "state-level FIPS code" and the state code for each beats is `53`. The "county-level FIPS code" is `53033` and the county code for each beat is `033`, though the only difference is that the "county-level FIPS code" includes both the state code and county code.

##### (g) Extracting 11-digit Codes

The census data uses an 11-digit code that consists of the state, county, and tract code. It does not include the block code. To join the census data to the Beats Dataset, we must have this code for each of the beats. Extract the 11-digit code for each of the beats in the Beats Dataset. The 11 digits consist of the 2 state digits, 3 county digits, and 6 tract digits. Add a column with the 11-digit code for each beat.

**Answer:**

Similar to above, we can make a function that gets a the 11-digit code when provided with a census code. Then we can apply that function to the `Census.Tract` column in the `beats_data` dataset and use mutate to add it as a new column.

```{r}
# Make function that gets the 11-digit code when provided a census code. First
# split the census code to separate each number. Then return the first three
# numbers in the census code.
get_11_digit_code <- function(census_tract_val) {
  split <- strsplit(census_tract_val, split = "")
  paste0(split[[1]][1:11], collapse = "") 
}

# Apply the `get_11_digit_code()` to each `Census.Tract` value in the
# `beats_data` dataset
eleven_digit_code <- lapply(beats_data$Census.Tract, FUN = get_11_digit_code)

# Add `get_11_digit_code` as a new column in `beats_data`
beats_data <- mutate(beats_data, Eleven.Digit.Code = eleven_digit_code)
```

##### (h) Extracting 11-digit Codes From Census

Now, we will examine census data  provided om `census_edu_data.csv`. The data includes counts of education attainment across different census tracts. Note how this data is in a 'wide' format and how it can be converted to a 'long' format. For now, we will work with it as is.

The census data contains a `GEO.id` column. Among other things, this variable encodes the 11-digit code that we had extracted above for each of the police beats. Specifically, when we look at the characters after the characters "US" for values of `GEO.id`, we see encodings for state, county, and tract, which should align with the beats we had above. Extract the 11-digit code from the `GEO.id` column. Add a column to the census data with the 11-digit code for each census observation.

**Answer:**

After loading the `census_edu_data.csv` file into the same folder as this markdown file, we can use the `read.csv()` function to load the data.

```{r}
census_edu_data <- read.csv("census_edu_data.csv", stringsAsFactors = FALSE)
```

Similar to above, we can make a function that gets a the 11-digit code when provided with a GEO ID code. Then we can apply that function to the `GEO.id` column in the `census_edu_data` dataset and use mutate to add it as a new column.

```{r}
# Make function that gets the 11-digit code when provided a GEO ID code. First
# split the GEO ID code to separate each number. Then return the tenth through
# last number in the GEO ID code.
census_get_11_digit_code <- function(census_tract_val) {
  split <- strsplit(census_tract_val, split = "")
  paste0(split[[1]][10:lengths(split)], collapse = "") 
}

# Apply the `census_get_11_digit_code()` to each `GEO.id` value in the
# `census_edu_data` dataset
eleven_digit_code <- lapply(census_edu_data$GEO.id,
                            FUN = census_get_11_digit_code)

# Add `eleven_digit_code` as new columns in `census_edu_data`
census_edu_data <- mutate(census_edu_data,
                          Eleven.Digit.Code = eleven_digit_code)
```

##### (i) Join Datasets

Join the census data with the Beat Dataset using the 11-digit codes as keys. Be sure that you do not lose any of the police beats when doing this join (i.e. your output dataframe should have the same number of rows as the cleaned Beats Dataset - use the correct join). Are there any police beats that do not have any associated census data? If so, how many?

Then, join the Crime Dataset to our joined beat/census data. We can do this using the police beat name. Again, be sure you do not lose any observations from the Crime Dataset. What is the final dimensions of the joined dataset?

Once everything is joined, save the final dataset for future use.

**Answer:**

Since we want to join all of the rows in `beats_data` to rows in `census_edu_data`, we can use a left-join with the `beats_data` dataset as the first input and `census_edu_data` as the second input, and joining by the column `Eleven.Digit.Code`. This will keep all of the rows in `beats_data` and only include corresponding rows in `census_edu_data`.

```{r}
# Use a left-join to all of the rows in `beats_data` with the corresponding rows
# in `census_edu_data`
beat_census_data <- left_join(beats_data, census_edu_data,
                              by = "Eleven.Digit.Code")

# Filter rows in `beat_census_data` to include only those that have `NA` values
na_val_beat_census <- beat_census_data %>%
  select(GEO.id:length(beat_census_data)) %>%
  filter(rowSums(is.na(beat_census_data)) != 0)

# Count the number of rows in `na_val_beat_census`
nrow(na_val_beat_census)
```

There are **24 police beats** that do not have any associated census data.

Since we want to join all of the rows in `trimmed_crimes_data_2011` to rows in `beat_census_data`, we can use a left-join with the `trimmed_crimes_data_2011` dataset as the first input and `beat_census_data` as the second input, and joining by the column `Beat` in `trimmed_crimes_data_2011` and `Name` in `beat_census_data`. This will keep all of the rows in `trimmed_crimes_data_2011` and only include corresponding rows in `beat_census_data`.

```{r}
# Use a left-join to all of the rows in `trimmed_crimes_data_2011` with the
# corresponding rows in `beat_census_data`
crime_beat_census_data <- left_join(trimmed_crimes_data_2011, beat_census_data,
                                    by = c("Beat" = "Name"))

# Find the dimensions in `crime_beat_census_data`
dim(crime_beat_census_data)

# Since `write.csv()` requires data to not be lists, we need to convert the
# `State.Code`, `County.Code`, and `Eleven.Digit.Code` columns into numerical
# data types
crime_beat_census_data$State.Code <-
  as.numeric(crime_beat_census_data$State.Code)
crime_beat_census_data$County.Code <-
  as.numeric(crime_beat_census_data$County.Code)
crime_beat_census_data$Eleven.Digit.Code <-
  as.numeric(crime_beat_census_data$Eleven.Digit.Code)

# Save the `crime_beat_census_data` dataset for future use
write.csv(crime_beat_census_data,
          "C:/Users/Owner/OneDrive/Documents/University of Washington/Graduate/year_1/autumn_2021/IMT_573/crime_beat_census_data.csv",
          row.names=FALSE)
```

There are **347,980 rows and 46 columns** in `crime_beat_census_data`.
